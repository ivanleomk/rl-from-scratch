# Topic 5: Q-Learning & Off-Policy Methods

## Overview

So far, we've focused on **on-policy** methods (REINFORCE, A2C, PPO), which learn from experience generated by the current policy. Now, we'll explore **off-policy** methods, which can learn from experience collected by any policy. This allows for greater sample efficiency.

The cornerstone of modern off-policy learning is **Deep Q-Networks (DQN)**.

## The Problem with On-Policy Learning

On-policy algorithms are inefficient. After each policy update, you have to throw away all the old data because it was generated by a different policy. This is like learning to play chess but throwing away all your old games after every new insight.

## Off-Policy Learning & Q-Functions

Off-policy methods learn a **Q-function**, `Q(s,a)`, which estimates the expected return of taking action `a` in state `s` and then following the optimal policy thereafter.

```
Q(s,a) = E[R_t | s_t = s, a_t = a]
```

If we have the optimal Q-function, `Q*(s,a)`, then the optimal policy is simple:

```
π*(s) = argmax_a Q*(s,a)
```

In plain English: "In any state, just take the action with the highest Q-value."

### The Bellman Equation

Q-learning is based on the Bellman equation, which provides a recursive relationship for the Q-function:

```
Q(s,a) = r + γ * max_{a'} Q(s', a')
```

This means the Q-value of the current state-action pair is the immediate reward `r` plus the discounted maximum Q-value of the next state `s'`.

We can turn this into a loss function by treating it as a regression problem:

- **Target**: `y = r + γ * max_{a'} Q(s', a')`
- **Prediction**: `Q(s,a)`
- **Loss**: `L = (y - Q(s,a))^2` (Mean Squared Error)

## Deep Q-Networks (DQN)

DQN uses a deep neural network to approximate the Q-function. However, naively applying the Bellman loss to a neural network is unstable. DQN introduced two key innovations to solve this.

### Innovation 1: Experience Replay

Instead of learning from consecutive samples, we store experience `(s, a, r, s')` in a large **replay buffer**. During training, we sample random minibatches from this buffer.

**Why this works**:
1. **Breaks correlations**: Consecutive samples are highly correlated. Random sampling breaks these correlations, making training more stable.
2. **Reuses data**: We can learn from the same experience multiple times, dramatically improving sample efficiency.

### Innovation 2: Target Networks

The Bellman loss creates a "moving target" problem. The same network is used to compute both the prediction `Q(s,a)` and the target `y`. As the network weights change, the target values also change, leading to oscillations and instability.

**Solution**: Use two networks.
1. **Main Network `Q_θ`**: This is the network we are actively training.
2. **Target Network `Q_θ'`**: A copy of the main network that is frozen for a period of time.

The target is computed using the target network:

`y = r + γ * max_{a'} Q_θ'(s', a')`

Every `N` steps, we copy the weights from the main network to the target network: `θ' ← θ`.

This keeps the target values stable for longer, preventing oscillations.

## The DQN Algorithm

1. Initialize main network `Q_θ` and target network `Q_θ'` with the same weights.
2. Initialize replay buffer `D`.
3. For each episode:
   - For each step `t`:
     - With probability `ε`, take a random action (explore).
     - Otherwise, take action `a_t = argmax_a Q_θ(s_t, a)` (exploit).
     - Execute action `a_t`, observe reward `r_t` and next state `s_{t+1}`.
     - Store transition `(s_t, a_t, r_t, s_{t+1})` in `D`.
     - Sample a random minibatch of transitions from `D`.
     - For each transition in the minibatch:
       - Compute target: `y = r + γ * max_{a'} Q_θ'(s', a')`
       - Compute loss: `L = (y - Q_θ(s,a))^2`
     - Update main network `Q_θ` by minimizing the loss.
     - Every `N` steps, update target network: `θ' ← θ`.

### Epsilon-Greedy Exploration

DQN uses a simple exploration strategy called **epsilon-greedy**. `ε` (epsilon) is the probability of taking a random action.

- Start with `ε = 1.0` (fully random).
- Gradually decrease `ε` to a small value like `0.01` (mostly greedy).

This ensures the agent explores at the beginning of training and exploits its knowledge later on.

## Extensions to DQN

Many improvements have been made to the original DQN algorithm:

- **Double DQN**: Reduces overestimation bias in Q-values.
- **Dueling DQN**: Separates the estimation of state value `V(s)` and advantage `A(s,a)`.
- **Prioritized Experience Replay (PER)**: Replays important transitions more frequently.
- **Rainbow**: Combines all these improvements into one algorithm.

## Common Issues & Debugging

### Issue: Q-values diverge

**Symptom**: Q-values grow to infinity.

**Fixes**:
- Use gradient clipping (clip the loss or gradients).
- Ensure target network is being updated correctly.
- Check reward scale (normalize if necessary).

### Issue: Agent doesn't learn

**Symptom**: Loss decreases, but agent performance doesn't improve.

**Fixes**:
- Check exploration schedule (`ε`). Is it exploring enough?
- Increase replay buffer size.
- Tune learning rate.

## Exercises

1. **Implement DQN from scratch**
   - Implement the Q-network.
   - Implement the experience replay buffer.
   - Implement the epsilon-greedy exploration strategy.
   - Implement the training loop with target networks.
   - Test on a classic control environment like CartPole or a simple Atari game like Pong.

2. **Compare On-Policy vs. Off-Policy**
   - Train DQN and PPO on the same environment.
   - Compare their sample efficiency (how many steps in the environment do they need to learn?).
   - DQN should be more sample-efficient.

3. **Experiment with Experience Replay**
   - Try different replay buffer sizes.
   - Try training without experience replay (learning online). What happens?

## Key Takeaways

- Off-policy methods like DQN can learn from old data, making them more sample-efficient.
- **Experience Replay** and **Target Networks** are crucial for stabilizing Q-learning with neural networks.
- DQN is a powerful algorithm for discrete action spaces.
- Understanding Q-learning is fundamental to many advanced RL topics.

## Links & Resources

- [Spinning Up: DQN](https://spinningup.openai.com/en/latest/spinningup/keypapers.html#deep-q-learning)
- [DQN Paper](https://arxiv.org/abs/1312.5602)
- [Blog: Let's make a DQN](https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/)

## Next Steps

DQN is great for discrete actions, but what about continuous actions like steering a car? In Topic 6, we'll explore algorithms like DDPG and SAC that extend these ideas to continuous control.
